=============== Sun Apr 03 10:16:08 EDT 2016
th LSTMN.lua -gpuid 0 -checkpoint runs/loaded_all_and_then_sick/cv4/model_model_epoch4.00_0.98.t7 -score_files "/home/vj/data/data-bank/textual-entailment/RTE/RTE1.test.csi.tokenized.txt /home/vj/data/data-bank/textual-entailment/RTE/RTE2.test.csi.tokenized.txt /home/vj/data/data-bank/textual-entailment/RTE/RTE3.test.csi.tokenized.txt"
using CUDA on GPU 0...	
for mode score restoring checkpoint from runs/loaded_all_and_then_sick/cv4/model_model_epoch4.00_0.98.t7	
restored.	
Processing text into tensors...	
vj: opening file	/home/vj/data/data-bank/textual-entailment/RTE/RTE1.test.csi.tokenized.txt	
vj: opening file	/home/vj/data/data-bank/textual-entailment/RTE/RTE2.test.csi.tokenized.txt	
vj: opening file	/home/vj/data/data-bank/textual-entailment/RTE/RTE3.test.csi.tokenized.txt	
(T,H) pair count: train 800, val 800, test 800	
Word vocab size: 45497	
data load done. Number of batches in train:	
File /home/vj/data/data-bank/textual-entailment/RTE/RTE1.test.csi.tokenized.txt 50	
File /home/vj/data/data-bank/textual-entailment/RTE/RTE2.test.csi.tokenized.txt 50	
File /home/vj/data/data-bank/textual-entailment/RTE/RTE3.test.csi.tokenized.txt 50	
number of parameters in the model: 31471806	
cloning dec	
cloning enc	
scoring...	
evaluating loss over split index 	1	
File /home/vj/data/data-bank/textual-entailment/RTE/RTE1.test.csi.tokenized.txt test_loss = 0.4412	
evaluating loss over split index 	2	
File /home/vj/data/data-bank/textual-entailment/RTE/RTE2.test.csi.tokenized.txt test_loss = 0.4662	
evaluating loss over split index 	3	
File /home/vj/data/data-bank/textual-entailment/RTE/RTE3.test.csi.tokenized.txt test_loss = 0.4925	
[vj@x10cuda SNLI-attention]$ 

=============== Sun Apr 03 08:27:11 EDT 2016
th LSTMN.lua -gpuid 0 -checkpoint "runs/loaded_all_and_then_sick/cv4/model_model_epoch4.00_0.98.t7" -score_files /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt
using CUDA on GPU 0...	
for mode score restoring checkpoint from runs/loaded_all_and_then_sick/cv4/model_model_epoch4.00_0.98.t7	
restored.	
Processing text into tensors...	
vj: opening file	/home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt	
(T,H) pair count: test 277	
Word vocab size: 45497	
data load done. Number of batches in train:	
File /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt 17	
number of parameters in the model: 31471806	
cloning dec	
cloning enc	
scoring...	
evaluating loss over split index 	1	
File /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt test_loss = 0.4485	
[vj@x10cuda SNLI-attention]$ th LSTMN.lua -gpuid 0 -checkpoint "runs/loaded_all_and_then_sick/cv4/model_model_epoch4.00_0.98.t7" -max_length 50 -score_files /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt
using CUDA on GPU 0...	
for mode score restoring checkpoint from runs/loaded_all_and_then_sick/cv4/model_model_epoch4.00_0.98.t7	
restored.	
Processing text into tensors...	
vj: opening file	/home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt	
(T,H) pair count: test 277	
Word vocab size: 45497	
data load done. Number of batches in train:	
File /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt 17	
number of parameters in the model: 31471806	
cloning dec	
cloning enc	
scoring...	
evaluating loss over split index 	1	
File /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt test_loss = 0.3860	
[vj@x10cuda SNLI-attention]$ th LSTMN.lua -gpuid 0 -checkpoint "runs/loaded_all_and_then_sick/cv4/model_model_epoch4.00_0.98.t7" -max_length 75 -score_files /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt
using CUDA on GPU 0...	
for mode score restoring checkpoint from runs/loaded_all_and_then_sick/cv4/model_model_epoch4.00_0.98.t7	
restored.	
Processing text into tensors...	
vj: opening file	/home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt	
(T,H) pair count: test 277	
Word vocab size: 45497	
data load done. Number of batches in train:	
File /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt 17	
number of parameters in the model: 31471806	
cloning dec	
cloning enc	
scoring...	
evaluating loss over split index 	1	
File /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt test_loss = 0.3787	
[vj@x10cuda SNLI-attention]$ th LSTMN.lua -gpuid 0 -checkpoint "runs/loaded_all_and_then_sick/cv4/model_model_epoch4.00_0.98.t7" -max_length 100 -score_files /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt
using CUDA on GPU 0...	
for mode score restoring checkpoint from runs/loaded_all_and_then_sick/cv4/model_model_epoch4.00_0.98.t7	
restored.	
Processing text into tensors...	
vj: opening file	/home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt	
(T,H) pair count: test 277	
Word vocab size: 45497	
data load done. Number of batches in train:	
File /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt 17	
number of parameters in the model: 31471806	
cloning dec	
cloning enc	
scoring...	
evaluating loss over split index 	1	
THCudaCheck FAIL file=/tmp/luarocks_cutorch-scm-1-2509/cutorch/lib/THC/generic/THCStorage.cu line=40 error=2 : out of memory
/opt/torch/install/bin/luajit: /opt/torch/install/share/lua/5.1/torch/Tensor.lua:374: cuda runtime error (2) : out of memory at /tmp/luarocks_cutorch-scm-1-2509/cutorch/lib/THC/generic/THCStorage.cu:40
stack traceback:
	[C]: in function 'resize'
	/opt/torch/install/share/lua/5.1/torch/Tensor.lua:374: in function 'repeatTensor'
	./util/ReplicateAdd.lua:13: in function 'func'
	/opt/torch/install/share/lua/5.1/nngraph/gmodule.lua:311: in function 'neteval'
	/opt/torch/install/share/lua/5.1/nngraph/gmodule.lua:346: in function 'forward'
	./util/model_utils.lua:197: in function 'eval_split'
	LSTMN.lua:177: in main chunk
	[C]: in function 'dofile'
	/opt/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
	[C]: at 0x004064d0
[vj@x10cuda SNLI-attention]$ 


=============== Sun Apr 03 08:13:48 EDT 2016

th LSTMN.lua -gpuid 0 -checkpoint runs/loaded_all_and_then_sick/cv4/model_model_epoch4.00_0.98.t7 -score_files /home/vj/data/data-bank/textual-entailment/compliance/compliance-TE-v1.all.csi.tokenized.txt

=============== Sun Apr 03 08:12:26 EDT 2016
th LSTMN.lua -gpuid 0 -checkpoint runs/loaded_all/cv4/model_model_epoch4.36_0.80.t7 -checkpoint_dir runs/loaded_all_and_then_sick/cv4/ -data_dir runs/loaded_all_and_then_sick/data/ 
using CUDA on GPU 0...	
for mode start restoring checkpoint from runs/loaded_all/cv4/model_model_epoch4.36_0.80.t7	
restored.	
Processing text into tensors...	
vj: opening file	runs/loaded_all_and_then_sick/data/train.txt	
vj: opening file	runs/loaded_all_and_then_sick/data/dev.txt	
vj: opening file	runs/loaded_all_and_then_sick/data/test.txt	
(T,H) pair count: train 4500, val 4500, test 4927	
loading word vecs...	
...loaded	
Word vocab size: 45497	
data load done. Number of batches in train:	
File runs/loaded_all_and_then_sick/data/train.txt 281	
File runs/loaded_all_and_then_sick/data/dev.txt 281	
File runs/loaded_all_and_then_sick/data/test.txt 307	
number of parameters in the model: 31471806	
cloning dec	
cloning enc	
vj: 1.0 opt.batch_size	16	 opt.rnn_size 	400	
starting training from ckp...	
vj before train 	  16
 400
[torch.LongStorage of size 2]

vj in train 	  16
 400
[torch.LongStorage of size 2]

start_iterations 	1	 iterations 	1124	
1000/1124 (epoch 3.56), train_loss = 0.0215	
evaluate on validation set h_init 	  16
 400
[torch.LongStorage of size 2]

evaluating loss over split index 	2	h_init:	nil	
/opt/torch/install/bin/luajit: ./util/model_utils.lua:162: model_utils: 161 h_init cannot be nil 
stack traceback:
	[C]: in function 'assert'
	./util/model_utils.lua:162: in function 'eval_split'
	./util/model_utils.lua:252: in function 'train'
	LSTMN.lua:193: in main chunk
	[C]: in function 'dofile'
	/opt/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
	[C]: at 0x004064d0
[vj@x10cuda SNLI-attention]$ 
using CUDA on GPU 0...	
for mode start restoring checkpoint from runs/loaded_all/cv4/model_model_epoch4.36_0.80.t7	
restored.	
Processing text into tensors...	
vj: opening file	runs/loaded_all_and_then_sick/data/train.txt	
vj: opening file	runs/loaded_all_and_then_sick/data/dev.txt	
vj: opening file	runs/loaded_all_and_then_sick/data/test.txt	
(T,H) pair count: train 4500, val 4500, test 4927	
loading word vecs...	
...loaded	
Word vocab size: 45497	
data load done. Number of batches in train:	
File runs/loaded_all_and_then_sick/data/train.txt 281	
File runs/loaded_all_and_then_sick/data/dev.txt 281	
File runs/loaded_all_and_then_sick/data/test.txt 307	
number of parameters in the model: 31471806	
cloning dec	
cloning enc	
starting training from ckp...	
start_iterations 	1	 iterations 	1124	
1000/1124 (epoch 3.56), train_loss = 0.0207	
evaluate on validation set 
evaluating loss over split index 	2	
0.97531138790036	
evaluate on test set 
test_accuracy = 0.7577	
[vj@x10cuda SNLI-attention]$ 


